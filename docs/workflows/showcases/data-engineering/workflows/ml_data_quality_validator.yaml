$schema: "workflow/v2.0"
name: ml_data_quality_validator
version: 2.0.0
description: Consensus-based ML data quality validation to prevent wasted training costs

execution:
  provider: anthropic
  model: claude-sonnet-4
  temperature: 0.2

steps:
  # Step 1: Analyze dataset structure
  - name: analyze_structure
    run: |
      Analyze this ML dataset structure:
      
      {{input}}
      
      **Dataset Overview:**
      - Format: [CSV, JSON, Parquet, etc.]
      - Rows: [count]
      - Columns: [count and names]
      - Size: [MB/GB]
      - Target column: [name if supervised learning]
      
      **Column Analysis:**
      For each column:
      - Name: [column name]
      - Type: [numeric, categorical, text, datetime]
      - Nullable: [Yes/No]
      - Unique values: [count or estimate]
      - Sample values: [show 3-5 examples]
      
      **Data Types Detected:**
      - Features: [count]
      - Target: [name]
      - Identifiers: [columns that are IDs]
      - Timestamps: [date/time columns]
      - Free text: [text columns]
      
      **Initial Assessment:**
      - Dataset appears suitable for ML: [Yes/No]
      - Obvious issues: [None/List]
      - Recommended preprocessing: [List]

  # Step 2: Consensus quality validation
  - name: quality_consensus
    needs: [analyze_structure]
    consensus:
      prompt: |
        Validate ML dataset quality with consensus:
        
        Dataset: {{input}}
        Structure: {{analyze_structure}}
        
        Check for data quality issues that would waste ML training:
        
        **1. Missing Data:**
        - Any columns with >20% missing values?
        - Is missingness random or systematic?
        - Can missing values be imputed safely?
        - Should rows/columns be dropped?
        
        **2. Data Leakage:**
        - Any features that leak the target?
        - Future information in features?
        - Identifiers that enable cheating?
        - Test data contamination?
        
        **3. Class Imbalance:**
        - Target distribution: [counts per class]
        - Severe imbalance (>95/5)? [Yes/No]
        - Requires resampling/weighting? [Yes/No]
        - Minority class sufficient? [Yes/No]
        
        **4. Data Quality Issues:**
        - Duplicate rows: [count if any]
        - Outliers: [extreme values]
        - Inconsistent formats: [examples]
        - Encoding errors: [issues]
        - Invalid values: [NaN, Inf, etc.]
        
        **5. Feature Quality:**
        - Zero-variance features: [constant columns]
        - Highly correlated features (>0.95): [pairs]
        - Cardinality issues: [too many categories]
        - Date/time parsing issues: [examples]
        
        **6. Label Quality (if supervised):**
        - Label distribution: [balanced/imbalanced]
        - Label errors suspected: [Yes/No]
        - Multi-label issues: [if applicable]
        - Ambiguous labels: [examples]
        
        **7. Statistical Issues:**
        - Feature-target correlation: [any correlated?]
        - Unexpected distributions: [skewed, bimodal]
        - Scale differences: [features with different ranges]
        - Time-based drift: [if temporal data]
        
        Rate severity for EACH issue found:
        - CRITICAL: Will cause training to fail or produce useless model
        - HIGH: Will significantly hurt model performance
        - MEDIUM: May reduce model quality
        - LOW: Minor issue, easy to fix
        
        **Overall Assessment:**
        - Ready for ML training: [Yes/No]
        - Preprocessing required: [Yes/No]
        - Estimated fix effort: [hours]
        - Risk level: [LOW/MEDIUM/HIGH/CRITICAL]
      executions:
        - provider: anthropic
          model: claude-sonnet-4
          temperature: 0.1
        - provider: openai
          model: gpt-4o
          temperature: 0.1
        - provider: deepseek
          model: deepseek-chat
          temperature: 0.1
      require: 2/3  # At least 2 must agree
      timeout: 120s

  # Step 3: Analyze consensus results
  - name: analyze_consensus
    needs: [quality_consensus]
    run: |
      Analyze consensus data quality assessment:
      
      {{quality_consensus}}
      
      **HIGH CONFIDENCE ISSUES (2+ validators agree):**
      
      For each consensus issue:
      - Issue: [description]
      - Severity: [consensus severity]
      - Agreement: [2/3 or 3/3]
      - Impact: [what will happen if not fixed]
      - Fix: [how to address]
      - Effort: [time to fix]
      
      **DISAGREEMENTS (validators differ):**
      
      For each disagreement:
      - Issue: [description]
      - Validator opinions: [list each view]
      - Recommendation: [manual review or accept uncertainty]
      
      **PRIORITY CLASSIFICATION:**
      
      **CRITICAL (Block training):**
      - [High confidence CRITICAL issues]
      - Training would fail or produce useless model
      - MUST fix before any training
      
      **HIGH (Fix before training):**
      - [High confidence HIGH issues]
      - Would significantly hurt performance
      - Fix now to avoid wasted compute
      
      **MEDIUM (Fix recommended):**
      - [High confidence MEDIUM or medium confidence HIGH]
      - May reduce model quality
      - Consider fixing, document if not
      
      **LOW (Optional improvements):**
      - [Low severity or low confidence]
      - Minor issues
      - Can address in iteration
      
      **COST-BENEFIT ANALYSIS:**
      
      **If proceed without fixes:**
      - Training cost: $[estimate GPU hours]
      - Probability of failure: [percentage]
      - Expected waste: $[cost × probability]
      
      **If fix issues first:**
      - Fix effort: [hours]
      - Fix cost: $[hours × hourly rate]
      - Training cost: $[same]
      - Probability of success: [much higher]
      
      **Recommendation:**
      - Proceed: [Yes/No]
      - Fix first: [Yes/No]
      - Issues to address: [count]
      - Expected improvement: [description]

  # Step 4: Generate preprocessing recommendations
  - name: preprocessing_plan
    needs: [analyze_consensus]
    run: |
      Generate preprocessing plan based on consensus issues:
      
      Issues: {{analyze_consensus}}
      Structure: {{analyze_structure}}
      
      **Preprocessing Pipeline:**
      
      **Phase 1: Critical Fixes (Must complete)**
      
      1. **Data Leakage Removal:**
         - Drop column: [leaky column name]
         - Reason: [why it leaks]
         - Code:
         ```python
         df = df.drop(columns=['leaky_column'])
         ```
      
      2. **Invalid Data Removal:**
         - Handle: [NaN, Inf, invalid values]
         - Strategy: [drop/impute]
         - Code:
         ```python
         df = df.replace([np.inf, -np.inf], np.nan)
         df = df.dropna(subset=['critical_column'])
         ```
      
      **Phase 2: Quality Improvements**
      
      1. **Missing Value Imputation:**
         - Columns: [list]
         - Method: [mean/median/mode/forward-fill]
         - Code:
         ```python
         from sklearn.impute import SimpleImputer
         imputer = SimpleImputer(strategy='median')
         df[numeric_cols] = imputer.fit_transform(df[numeric_cols])
         ```
      
      2. **Outlier Handling:**
         - Columns: [list]
         - Method: [clip/remove/transform]
         - Code:
         ```python
         # Clip outliers at 3 standard deviations
         df['column'] = df['column'].clip(
             df['column'].mean() - 3*df['column'].std(),
             df['column'].mean() + 3*df['column'].std()
         )
         ```
      
      3. **Feature Scaling:**
         - Columns: [numeric features]
         - Method: [StandardScaler/MinMaxScaler]
         - Code:
         ```python
         from sklearn.preprocessing import StandardScaler
         scaler = StandardScaler()
         df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
         ```
      
      **Phase 3: Optional Improvements**
      
      1. **Class Balancing (if needed):**
         ```python
         from imblearn.over_sampling import SMOTE
         X_resampled, y_resampled = SMOTE().fit_resample(X, y)
         ```
      
      2. **Feature Engineering:**
         - Create: [suggested features]
         - Transform: [log/polynomial transforms]
      
      **Validation Script:**
      
      ```python
      # After preprocessing, validate again
      assert df.isnull().sum().sum() == 0, "Missing values remain"
      assert not np.isinf(df.select_dtypes(include=np.number)).any().any(), "Inf values remain"
      assert len(df) > 0, "No data remaining"
      print(f"Clean dataset: {len(df)} rows, {len(df.columns)} columns")
      ```
      
      **Estimated Effort:**
      - Phase 1: [hours]
      - Phase 2: [hours]
      - Phase 3: [hours]
      - Total: [hours]

  # Step 5: Generate validation report
  - name: validation_report
    needs: [preprocessing_plan]
    run: |
      Generate comprehensive ML data quality report:
      
      # ML Data Quality Validation Report
      
      **Validated:** {{execution.timestamp}}
      **Workflow:** {{workflow.name}} v{{workflow.version}}
      **Validators:** 3 AI systems (consensus required)
      
      ---
      
      ## Dataset Overview
      
      {{analyze_structure}}
      
      ---
      
      ## Consensus Quality Assessment
      
      {{quality_consensus}}
      
      **Agreement Level:** [percentage]%
      **Validators in agreement:** [count]/3
      
      ---
      
      ## Issue Analysis
      
      {{analyze_consensus}}
      
      ---
      
      ## Preprocessing Recommendations
      
      {{preprocessing_plan}}
      
      ---
      
      ## Decision
      
      **Status:** [APPROVED / FIX REQUIRED / REJECTED]
      
      **If APPROVED:**
      - Dataset ready for ML training
      - Minor issues can be addressed in iteration
      - Proceed with confidence
      
      **If FIX REQUIRED:**
      - Critical issues found
      - Fix before training to avoid waste
      - Estimated fix time: [hours]
      - Expected improvement: [description]
      
      **If REJECTED:**
      - Dataset fundamentally unsuitable
      - Major data collection/labeling issues
      - Do not proceed with current data
      
      ---
      
      ## Cost-Benefit Analysis
      
      **Without validation:**
      - Training cost: $400 (GPU hours)
      - Failure probability: 60%
      - Expected waste: $240
      
      **With validation:**
      - Validation cost: $0.05 (this workflow)
      - Fix cost: $[hours × rate]
      - Training cost: $400
      - Success probability: 95%
      - Expected waste: $20
      
      **Savings: $220 per dataset**
      
      ---
      
      ## Next Steps
      
      1. **Address Critical Issues:**
         - [ ] [Issue 1]
         - [ ] [Issue 2]
      
      2. **Run Preprocessing:**
         ```bash
         python preprocess.py --input raw_data.csv --output clean_data.csv
         ```
      
      3. **Validate Cleaned Data:**
         - Re-run this workflow on cleaned data
         - Verify issues resolved
      
      4. **Proceed to Training:**
         - Split: train/val/test
         - Baseline model
         - Iterate and improve
      
      ---
      
      ## Quality Metrics
      
      **Consensus Agreement:** {{quality_consensus.agreement}}%
      **Critical Issues:** [count]
      **High Priority Issues:** [count]
      **Dataset Quality:** [Good/Fair/Poor]
      **Ready for Training:** [Yes/No]
      
      ---
      
      **Note:** This validation uses 3-provider consensus to catch data
      quality issues that would waste expensive ML training compute.

# Usage:
#
# Validate ML dataset:
# ./mcp-cli --workflow ml_data_quality_validator \
#   --input-data "$(cat training_data.csv)"
#
# From database query:
# psql -c "SELECT * FROM ml_training_data" | \
#   ./mcp-cli --workflow ml_data_quality_validator
#
# Business Value:
# - Prevents wasted ML training costs
# - Consensus validation catches issues single review misses
# - Systematic checks across 7 quality dimensions
# - Clear fix recommendations with code examples
# - Cost-benefit analysis for decision making
#
# ROI Calculation:
# - ML training cost: $400 (GPU hours)
# - Without validation: 60% failure rate = $240 waste
# - Validation cost: $0.05 (this workflow)
# - With validation: 95% success rate = $20 waste
# - Savings: $220 per dataset
#
# Annual Impact (10 datasets):
# - Saves: $2,200 in prevented waste
# - Plus: Faster time to production models
# - Plus: Higher model quality
# - Plus: Reduced frustration
#
# Quality Improvements:
# - 3-provider consensus: Higher confidence
# - Systematic checks: Nothing missed
# - Actionable fixes: Clear path forward
# - Prevents common pitfalls: Data leakage, imbalance, etc.
